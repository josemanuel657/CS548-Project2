{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 1 - Dataset, Background, Significance, Motivation.\n",
    "\n",
    "For this project, I have selected the [soccer](https://www.kaggle.com/datasets/davidcariboo/player-scores?select=transfers.csv) dataset, which is publicly available on Kaggle. \n",
    "\n",
    "\n",
    "## Background and Significance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Motivation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 -Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Player mean and red cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Feature Engineering, Data Transformation, and Cleaning\n",
    "\n",
    "In this phase, we prepare the dataset for accurate match outcome predictions by engineering features, transforming raw data, selecting relevant variables, merging multiple datasets, and  cleaning the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Player Data with Additional Features\n",
    "\n",
    "This step focuses on improving the player dataset by adding performance metrics.\n",
    "\n",
    "1. **Loading and Merging Player Data**: Player appearances, details, valuations, and game lineups are loaded and merged. The most recent valuations and positions are extracted to ensure we have up-to-date data for each player.\n",
    "\n",
    "2. **Aggregating Player Statistics**: We aggregate player stats such as goals, assists, yellow and red cards, and minutes played. Players with fewer than 1000 minutes are filtered out.\n",
    "\n",
    "3. **Per-Minute Performance Metrics**: Goals, assists, and cards are normalized by minutes played, and extreme values are capped for better visualization.\n",
    "\n",
    "4. **Performance Score Calculation**: A weighted performance score is computed (60% for goals, 30% for assists, 10% for cards) to rank players based on their contributions.\n",
    "\n",
    "5. **Merging Additional Features**: Recent valuations and positions are merged into the dataset, finalizing the player stats for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the appearance, player, and valuation data\n",
    "df_appearances = pd.read_csv(\"./data/appearances.csv\")\n",
    "df_players = pd.read_csv(\"./data/players.csv\")\n",
    "df_valuations = pd.read_csv(\"./data/player_valuations.csv\")  \n",
    "df_valuations['date'] = pd.to_datetime(df_valuations['date'])  \n",
    "df_game_lineups = pd.read_csv(\"./data/game_lineups.csv\")\n",
    "\n",
    "# Extract the most recent valuation for each player\n",
    "df_recent_valuations = df_valuations.sort_values(by='date').groupby('player_id').tail(1)\n",
    "df_recent_position = df_game_lineups.sort_values(by='date').groupby('player_id').tail(1)[['player_id', 'position']]  \n",
    "\n",
    "# Merge the filtered appearances data with the players data to include league info\n",
    "df_merged = pd.merge(df_appearances, df_players[['player_id', 'current_club_domestic_competition_id','current_club_id', 'name']], on='player_id')\n",
    "\n",
    "# Group by 'player_id', 'name', and 'current_club_domestic_competition_id' (league) and sum the relevant stats\n",
    "player_stats = df_merged.groupby(['player_id', 'name', 'current_club_domestic_competition_id', 'current_club_id']).agg({\n",
    "    'yellow_cards': 'sum',\n",
    "    'red_cards': 'sum',\n",
    "    'goals': 'sum',\n",
    "    'assists': 'sum',\n",
    "    'minutes_played': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Filter out players who have played fewer than 11-12 full matches\n",
    "min_minutes_threshold = 1000\n",
    "player_stats = player_stats[player_stats['minutes_played'] >= min_minutes_threshold]\n",
    "\n",
    "# Calculate the average yellow cards, red cards, goals, and assists per minute played\n",
    "player_stats['yellow_cards_per_minute'] = player_stats['yellow_cards'] / player_stats['minutes_played']\n",
    "player_stats['red_cards_per_minute'] = player_stats['red_cards'] / player_stats['minutes_played']\n",
    "player_stats['goals_per_minute'] = player_stats['goals'] / player_stats['minutes_played']\n",
    "player_stats['assists_per_minute'] = player_stats['assists'] / player_stats['minutes_played']\n",
    "\n",
    "# Cap extreme values for better visualization\n",
    "player_stats['yellow_cards_per_minute'] = player_stats['yellow_cards_per_minute'].clip(upper=0.05)\n",
    "player_stats['red_cards_per_minute'] = player_stats['red_cards_per_minute'].clip(upper=0.05)\n",
    "player_stats['goals_per_minute'] = player_stats['goals_per_minute'].clip(upper=0.05)\n",
    "player_stats['assists_per_minute'] = player_stats['assists_per_minute'].clip(upper=0.05)\n",
    "\n",
    "# Calculate per-minute stats for each match\n",
    "df_merged['goals_per_minute'] = df_merged['goals'] / df_merged['minutes_played']\n",
    "df_merged['assists_per_minute'] = df_merged['assists'] / df_merged['minutes_played']\n",
    "df_merged['yellow_cards_per_minute'] = df_merged['yellow_cards'] / df_merged['minutes_played']\n",
    "df_merged['red_cards_per_minute'] = df_merged['red_cards'] / df_merged['minutes_played']\n",
    "\n",
    "# Group by 'player_id' and calculate the standard deviation and mean for per-minute stats\n",
    "player_variance_stats_per_minute = df_merged.groupby('player_id').agg({\n",
    "    'goals_per_minute': ['std', 'mean'],\n",
    "    'assists_per_minute': ['std', 'mean'],\n",
    "    'yellow_cards_per_minute': ['std', 'mean'],\n",
    "    'red_cards_per_minute': ['std', 'mean']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the column names\n",
    "player_variance_stats_per_minute.columns = ['player_id', 'goals_per_minute_std', 'goals_per_minute_mean',\n",
    "                                            'assists_per_minute_std', 'assists_per_minute_mean',\n",
    "                                            'yellow_cards_per_minute_std', 'yellow_cards_per_minute_mean',\n",
    "                                            'red_cards_per_minute_std', 'red_cards_per_minute_mean']\n",
    "\n",
    "# Merge the variance stats per minute with the original player_stats\n",
    "player_stats = pd.merge(player_stats, player_variance_stats_per_minute, on='player_id')\n",
    "\n",
    "# Merge the most recent player valuations into the player_stats dataframe\n",
    "player_stats = pd.merge(player_stats, df_recent_valuations[['player_id', 'market_value_in_eur']], on='player_id')\n",
    "\n",
    "# Merge the most recent player position into the player_stats dataframe\n",
    "player_stats = pd.merge(player_stats, df_recent_position, on='player_id', how='left')\n",
    "\n",
    "# Define a ranking metric (performance score)\n",
    "player_stats['performance_score'] = (\n",
    "    player_stats['goals_per_minute'] * 0.6 +  \n",
    "    player_stats['assists_per_minute'] * 0.3 +  \n",
    "    player_stats['yellow_cards_per_minute'] * 0.1  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation: Player Position Mapping\n",
    "\n",
    "In this step, we map specific player positions to broader categories: 'Goalkeeper', 'Defenders', 'Midfielders', and 'Attackers'. This transformation makes it easier to analyze player roles across teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the position mapping\n",
    "position_mapping = {\n",
    "    'Goalkeeper': ['Goalkeeper'],\n",
    "    'Defenders': ['Centre-Back', 'Left-Back', 'Right-Back', 'Sweeper', 'Defensive Midfield'],\n",
    "    'Midfielders': ['Central Midfield', 'Left Midfield', 'Right Midfield', 'Attacking Midfield', 'midfield'],\n",
    "    'Attackers': ['Centre-Forward', 'Left Winger', 'Right Winger', 'Second Striker', 'Attack']\n",
    "}\n",
    "\n",
    "# Function to map positions\n",
    "def map_position(pos):\n",
    "    for key, values in position_mapping.items():\n",
    "        if pos in values:\n",
    "            return key\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Apply the mapping\n",
    "player_stats['position'] = player_stats['position'].apply(map_position)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection: Player Data, Team Data, Competitions Data, and Match Most Relevant Features\n",
    "\n",
    "In this step, to reduce complexity, we identify the most important features from player, team, competitions, and match data to create a dataset targeted at predicting match outcomes.\n",
    "\n",
    "1. **Player Data**: From the previous feature engineering.\n",
    "   - **goals**, **yellow cards**, and **red cards**\n",
    "   - **performance score**\n",
    "   - **market value in euros**\n",
    "   - **position**\n",
    "   - **current club ID**\n",
    "\n",
    "2. **Team Data**: From the club dataset.\n",
    "   - **domestic competition ID**\n",
    "   - **average age**\n",
    "   - **foreigners percentage**\n",
    "   - **stadium name**\n",
    "   - **coach name**\n",
    "   - **net transfer record** and **total market value**\n",
    "\n",
    "3. **Competitions Data**: From the competitions dataset.\n",
    "   - **competition ID**\n",
    "   - **country ID**\n",
    "\n",
    "4. **Match Data**: From the games dataset\n",
    "   - **home and away team statistics**\n",
    "   - **game outcomes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {\n",
    "    \"appearances\": './data/appearances.csv',\n",
    "    \"club_games\": './data/club_games.csv',\n",
    "    \"clubs\": './data/clubs.csv',\n",
    "    \"competitions\": './data/competitions.csv',\n",
    "    \"game_events\": './data/game_events.csv',\n",
    "    \"game_lineups\": './data/game_lineups.csv',\n",
    "    \"games\": './data/games.csv',\n",
    "    \"player_valuations\": './data/player_valuations.csv',\n",
    "    \"players\": './data/players.csv',\n",
    "    \"transfers\": './data/transfers.csv'\n",
    "}\n",
    "\n",
    "dfs = {key: pd.read_csv(path) for key, path in data_files.items()}\n",
    "\n",
    "games_df = dfs['games']\n",
    "player_stats = player_stats[['player_id', 'goals', 'yellow_cards', 'red_cards', 'performance_score', 'market_value_in_eur', 'position', 'current_club_id']]\n",
    "clubs_df = dfs['clubs'][['club_id', 'domestic_competition_id', 'average_age', 'foreigners_percentage', 'stadium_name', 'coach_name','net_transfer_record','total_market_value']]\n",
    "competitions_df = dfs['competitions'][['competition_id', 'country_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Merging: Relevant Player, Team, and Competition Features for Each Match\n",
    "\n",
    "In this step, we merge data from multiple sources—player performance, team statistics, and competition details—to build a rich and complete view of every match for match outcome prediction. To account for player details, we include 10 players per team in each match, ordered by position: 1 goalkeeper, 3 defenders, 3 midfielders, and 3 attackers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Merging club data\n",
    "home_club_df = clubs_df.rename(columns=lambda x: f\"home_team_{x}\")\n",
    "away_club_df = clubs_df.rename(columns=lambda x: f\"away_team_{x}\")\n",
    "merged_df = pd.merge(games_df, home_club_df, left_on='home_club_id', right_on='home_team_club_id', how='left')\n",
    "merged_df = pd.merge(merged_df, away_club_df, left_on='away_club_id', right_on='away_team_club_id', how='left')\n",
    "\n",
    "## Merging competition data\n",
    "home_competition_df = competitions_df.rename(columns=lambda x: f\"home_team_{x}\")\n",
    "away_competition_df = competitions_df.rename(columns=lambda x: f\"away_team_{x}\")\n",
    "merged_df = pd.merge(merged_df, home_competition_df, left_on='home_team_domestic_competition_id', right_on='home_team_competition_id', how='left')\n",
    "merged_df = pd.merge(merged_df, away_competition_df, left_on='away_team_domestic_competition_id', right_on='away_team_competition_id', how='left')\n",
    "\n",
    "# Define the top player count per position category\n",
    "top_counts = {\n",
    "    'Goalkeeper': 1,\n",
    "    'Defenders': 3,\n",
    "    'Midfielders': 3,\n",
    "    'Attackers': 3\n",
    "}\n",
    "\n",
    "# Mapping of positions to numeric values\n",
    "position_mapping_numeric = {'Goalkeeper': 1, 'Defenders': 2, 'Midfielders': 3, 'Attackers': 4}\n",
    "\n",
    "# Function to get top players for home and away teams\n",
    "def get_top_players_optimized(df, merged_df, top_counts):\n",
    "\n",
    "    home_club_ids = merged_df['home_club_id'].unique()\n",
    "    away_club_ids = merged_df['away_club_id'].unique()\n",
    "    \n",
    "    home_players = df[df['current_club_id'].isin(home_club_ids)]\n",
    "    away_players = df[df['current_club_id'].isin(away_club_ids)]\n",
    "\n",
    "    player_data_list = []\n",
    "\n",
    "    for i, row in merged_df.iterrows():\n",
    "\n",
    "        home_top_players = home_players[home_players['current_club_id'] == row['home_club_id']]\n",
    "        away_top_players = away_players[away_players['current_club_id'] == row['away_club_id']]\n",
    "\n",
    "        home_player_data = assign_top_players(home_top_players, 'home', i, top_counts)\n",
    "        away_player_data = assign_top_players(away_top_players, 'away', i, top_counts)\n",
    "        \n",
    "        player_data_list.append({**home_player_data, **away_player_data})\n",
    "    \n",
    "    player_data_df = pd.DataFrame(player_data_list)\n",
    "\n",
    "    merged_df = pd.concat([merged_df, player_data_df], axis=1)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Function to assign top players to columns\n",
    "def assign_top_players(top_players, prefix, row_index, top_counts):\n",
    "    positions = ['Goalkeeper', 'Defenders', 'Midfielders', 'Attackers']\n",
    "    top_players_list = [\n",
    "        top_players[top_players['position'] == position].nlargest(top_counts[position], 'performance_score')\n",
    "        for position in positions if position in top_players['position'].values\n",
    "    ]\n",
    "    \n",
    "    top_players = pd.concat(top_players_list, ignore_index=True) if top_players_list else pd.DataFrame()\n",
    "    \n",
    "    player_data = {}\n",
    "    \n",
    "    for i, player in enumerate(top_players.itertuples(), 1):\n",
    "        player_data[f'{prefix}_player_{i}_id'] = player.player_id\n",
    "        player_data[f'{prefix}_player_{i}_goals'] = player.goals\n",
    "        player_data[f'{prefix}_player_{i}_yellow_cards'] = player.yellow_cards\n",
    "        player_data[f'{prefix}_player_{i}_red_cards'] = player.red_cards\n",
    "        player_data[f'{prefix}_player_{i}_performance_score'] = player.performance_score\n",
    "        player_data[f'{prefix}_player_{i}_market_value_in_eur'] = player.market_value_in_eur\n",
    "        player_data[f'{prefix}_player_{i}_position'] = player.position\n",
    "        player_data[f'{prefix}_player_{i}_current_club_id'] = player.current_club_id\n",
    "        \n",
    "        if i == 10:  \n",
    "            break\n",
    "\n",
    "    return player_data\n",
    "\n",
    "## Merging player data\n",
    "merged_df = get_top_players_optimized(player_stats, merged_df, top_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Preparing for analysis\n",
    "\n",
    "This step focuses on preparing the dataset for match-level predictions ML nodels.\n",
    "\n",
    "1. **Dropping Unnecessary Columns**:\n",
    "   I removed irrelevant columns such as coach names and club IDs that were either duplicated or not useful for predictions.\n",
    "\n",
    "2. **Handling Missing Values**:\n",
    "   I filled missing values for key columns like player positions, manager names, and stadiums with default values like 'Unknown'. For numerical columns like attendance and team average age, I used the median value to fill gaps.\n",
    "\n",
    "3. **Converting Monetary Values**:\n",
    "   I converted transfer record data from strings (e.g., \"€1.5m\") to numerical values to standardize the dataset and filled any missing transfer records with the median value.\n",
    "\n",
    "4. **Handling Missing Player and Competition Data**:\n",
    "   Missing values in numeric player-related columns were filled with the mean, while non-numeric and competition-related columns were filled with defaults such as 'Unknown' or 0.\n",
    "\n",
    "5. **Creating Match Outcome Labels**:\n",
    "   I defined the match outcome (win, lose, or tie) based on the number of goals scored by the home and away teams. This serves as the target variable for the prediction model.\n",
    "\n",
    "6. **Validation**:\n",
    "   After processing, I validated the data by checking for any remaining missing values to ensure completeness.\n",
    "\n",
    "7. **Saving the Cleaned Data**:\n",
    "   Finally, I saved the cleaned dataframe for further analysis and model training, confirming that the match outcome and other key features were correctly processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/v72c8nlj6sx7p5pyh32jy7_c0000gn/T/ipykernel_77144/3107428121.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  cleaned_df['match_outcome'] = cleaned_df.apply(lambda row: determine_match_outcome(row['home_club_goals'], row['away_club_goals']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Drop unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'url', \n",
    "    'home_team_coach_name', 'away_team_coach_name',  \n",
    "    'home_team_club_id', 'away_team_club_id',        \n",
    "    'home_club_name', 'away_club_name',            \n",
    "    'aggregate'\n",
    "]\n",
    "cleaned_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "\n",
    "# Fill missing positions, manager names, formations, stadiums, and referee with 'Unknown' or a default value\n",
    "cleaned_df.fillna({\n",
    "    'home_club_position': 7,\n",
    "    'away_club_position': 7, # 7 is the default value for unknown. Usually leagues have 15 positions so 7 is the middle\n",
    "    'home_club_manager_name': 'Unknown',\n",
    "    'away_club_manager_name': 'Unknown',\n",
    "    'stadium': 'Unknown',\n",
    "    'home_team_stadium_name': 'Unknown',\n",
    "    'away_team_stadium_name': 'Unknown',\n",
    "    'home_club_formation': 'Unknown',\n",
    "    'away_club_formation': 'Unknown',\n",
    "    'referee': 'Unknown'\n",
    "}, inplace=True)\n",
    "\n",
    "# Fill missing attendance, average age, and foreigners percentage with the median\n",
    "cleaned_df['attendance'] = cleaned_df['attendance'].fillna(cleaned_df['attendance'].median())\n",
    "cleaned_df['home_team_average_age'] = cleaned_df['home_team_average_age'].fillna(cleaned_df['home_team_average_age'].median())\n",
    "cleaned_df['away_team_average_age'] = cleaned_df['away_team_average_age'].fillna(cleaned_df['away_team_average_age'].median())\n",
    "cleaned_df['home_team_foreigners_percentage'] = cleaned_df['home_team_foreigners_percentage'].fillna(cleaned_df['home_team_foreigners_percentage'].median())\n",
    "cleaned_df['away_team_foreigners_percentage'] = cleaned_df['away_team_foreigners_percentage'].fillna(cleaned_df['away_team_foreigners_percentage'].median())\n",
    "\n",
    "# Step 3: Convert monetary values for transfer redords\n",
    "def convert_monetary_value(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.replace('€', '').replace('+', '').replace(',', '')\n",
    "        if 'm' in value:\n",
    "            return float(value.replace('m', '')) * 1e6\n",
    "        elif 'k' in value:\n",
    "            return float(value.replace('k', '')) * 1e3\n",
    "        else:\n",
    "            return float(value)\n",
    "    return value\n",
    "\n",
    "cleaned_df['home_team_net_transfer_record'] = cleaned_df['home_team_net_transfer_record'].apply(convert_monetary_value)\n",
    "cleaned_df['away_team_net_transfer_record'] = cleaned_df['away_team_net_transfer_record'].apply(convert_monetary_value)\n",
    "\n",
    "# Fill missing values in transfer records and team market value with the median\n",
    "cleaned_df['home_team_net_transfer_record'] = cleaned_df['home_team_net_transfer_record'].fillna(cleaned_df['home_team_net_transfer_record'].median())\n",
    "cleaned_df['away_team_net_transfer_record'] = cleaned_df['away_team_net_transfer_record'].fillna(cleaned_df['away_team_net_transfer_record'].median())\n",
    "cleaned_df = cleaned_df.drop(['home_team_total_market_value', 'away_team_total_market_value'], axis=1)\n",
    "\n",
    "# Step 4: Handle missing values in player and competition columns\n",
    "\n",
    "# Fill missing player-related columns (numeric ones) with their mean\n",
    "player_columns = [col for col in cleaned_df.columns if 'player_' in col]\n",
    "for col in player_columns:\n",
    "    if pd.api.types.is_numeric_dtype(cleaned_df[col]):\n",
    "        cleaned_df[col] = cleaned_df[col].fillna(cleaned_df[col].mean())\n",
    "\n",
    "# Fill remaining missing player-related columns and position-related columns with 0\n",
    "player_columns = [col for col in cleaned_df.columns if 'player_' in col or 'position' in col]\n",
    "cleaned_df[player_columns] = cleaned_df[player_columns].fillna(0)\n",
    "\n",
    "# Fill missing competition-related columns with 'Unknown'\n",
    "competition_columns = [\n",
    "    'home_team_domestic_competition_id', 'away_team_domestic_competition_id',\n",
    "    'home_team_competition_id', 'away_team_competition_id',\n",
    "    'home_team_country_id', 'away_team_country_id'\n",
    "]\n",
    "cleaned_df[competition_columns] = cleaned_df[competition_columns].fillna('Unknown')\n",
    "\n",
    "# Step 5: Create the match outcome column (win_team_1, lose_team_1, tie)\n",
    "\n",
    "def determine_match_outcome(home_goals, away_goals):\n",
    "    if home_goals > away_goals:\n",
    "        return 'win_team_1'  # Home team wins\n",
    "    elif home_goals < away_goals:\n",
    "        return 'lose_team_1'  # Away team wins\n",
    "    else:\n",
    "        return 'tie'  # Match tied\n",
    "\n",
    "# Apply the outcome function to create the new column\n",
    "cleaned_df['match_outcome'] = cleaned_df.apply(lambda row: determine_match_outcome(row['home_club_goals'], row['away_club_goals']), axis=1)\n",
    "\n",
    "# Step 6: Validate the cleaning process by checking for missing values\n",
    "missing_columns = cleaned_df.isnull().sum()\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_columns = missing_columns[missing_columns > 0]\n",
    "\n",
    "# Step 7: Save the cleaned dataframe if necessary\n",
    "cleaned_df.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/df/v72c8nlj6sx7p5pyh32jy7_c0000gn/T/ipykernel_84748/833328674.py:7: DtypeWarning: Columns (119,127,135,143,151,159,167,175,183,191) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('cleaned_data.csv')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "# Preprocess only the categorical feature columns \n",
    "categorical_columns = df.drop(['match_outcome'], axis=1).select_dtypes(include=['object']).columns.tolist()\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column].astype(str))\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(['home_club_goals', 'away_club_goals', 'match_outcome'], axis=1)  # Drop the goal columns and target column\n",
    "y = df['match_outcome']  # Target is the match outcome (win_team_1, lose_team_1, tie)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the reusable evaluation function\n",
    "def evaluate_model(model_name, y_test, y_pred, label_encoder):\n",
    "    print(f\"{model_name} Model Evaluation:\\n\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.48      0.49      0.49      6907\n",
      "         tie       0.28      0.29      0.28      4480\n",
      "  win_team_1       0.58      0.57      0.57      9504\n",
      "\n",
      "    accuracy                           0.48     20891\n",
      "   macro avg       0.45      0.45      0.45     20891\n",
      "weighted avg       0.48      0.48      0.48     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Train a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict outcomes for the test set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Decision Tree model\n",
    "evaluate_model(\"Decision Tree\", y_test, y_pred_dt, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Information Gain (Via Entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Information Gain) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.54      0.55      0.54      6907\n",
      "         tie       0.29      0.16      0.20      4480\n",
      "  win_team_1       0.59      0.71      0.64      9504\n",
      "\n",
      "    accuracy                           0.54     20891\n",
      "   macro avg       0.47      0.47      0.46     20891\n",
      "weighted avg       0.51      0.54      0.52     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model with Information Gain\n",
    "rf_gini = RandomForestClassifier(criterion='entropy', n_estimators=100, random_state=42)\n",
    "rf_gini.fit(X_train, y_train)\n",
    "\n",
    "# Predict outcomes for the test set\n",
    "y_pred_gini = rf_gini.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "evaluate_model(\"Random Forest (Information Gain)\", y_test, y_pred_gini, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (Gini Index) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.53      0.54      0.53      6907\n",
      "         tie       0.29      0.16      0.21      4480\n",
      "  win_team_1       0.59      0.70      0.64      9504\n",
      "\n",
      "    accuracy                           0.53     20891\n",
      "   macro avg       0.47      0.47      0.46     20891\n",
      "weighted avg       0.50      0.53      0.51     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest with Gini Index\n",
    "rf_gini = RandomForestClassifier(criterion='gini', n_estimators=100, random_state=42)\n",
    "\n",
    "# Predict outcomes for the test set\n",
    "rf_gini.fit(X_train, y_train)\n",
    "y_pred_gini = rf_gini.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "evaluate_model(\"Random Forest (Gini Index)\", y_test, y_pred_gini, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with Gini Index and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best parameters found:  {'bootstrap': True, 'max_depth': 7, 'min_samples_leaf': 3, 'min_samples_split': 4, 'n_estimators': 137} \n",
      "\n",
      "Random Forest (Gini Index and hyperparameter tuning) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.57      0.55      0.56      6907\n",
      "         tie       0.91      0.01      0.02      4480\n",
      "  win_team_1       0.57      0.85      0.69      9504\n",
      "\n",
      "    accuracy                           0.57     20891\n",
      "   macro avg       0.69      0.47      0.42     20891\n",
      "weighted avg       0.64      0.57      0.50     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Set hyperparameter distribution for Random Forest (Asked chatGPT)\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# Perform Randomized Search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_distributions, n_iter=10,\n",
    "                                   scoring='accuracy', cv=2, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the random search\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params, \"\\n\")\n",
    "\n",
    "# Train the best model\n",
    "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance Pruning\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Select top 100 important features\n",
    "top_n = 100\n",
    "important_feature_indices = feature_importances.argsort()[-top_n:][::-1]\n",
    "selected_features = X.columns[important_feature_indices]\n",
    "\n",
    "# Apply RFE on the reduced feature set\n",
    "X_train_top = X_train[selected_features]\n",
    "X_test_top = X_test[selected_features]\n",
    "\n",
    "rf_model_rfe = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "# Select top 30 features\n",
    "rfe = RFE(rf_model_rfe, n_features_to_select=30)  \n",
    "X_train_rfe = rfe.fit_transform(X_train_top, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_top)\n",
    "\n",
    "# Retraining model with RFE-selected features\n",
    "best_rf_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = best_rf_model.predict(X_test_rfe)\n",
    "\n",
    "evaluate_model(\"Random Forest (Gini Index and hyperparameter tuning)\", y_test, y_pred, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with Hyperparameter Tuning and Importance Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.8, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 58, 'subsample': 1.0}\n",
      "XGBoost (with Hyperparameter Tuning and Importance Pruning) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.56      0.66      0.61      6907\n",
      "         tie       0.50      0.06      0.10      4480\n",
      "  win_team_1       0.62      0.80      0.70      9504\n",
      "\n",
      "    accuracy                           0.60     20891\n",
      "   macro avg       0.56      0.51      0.47     20891\n",
      "weighted avg       0.58      0.60      0.54     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Set up hyperparameter distribution for XGBoost (Asked chatGPT)\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'gamma': [0, 0.1]  \n",
    "}\n",
    "\n",
    "# Initialize the XGBClassifier\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Perform Randomized Search for hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_distributions, n_iter=10,\n",
    "                                   scoring='accuracy', cv=2, verbose=1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the random search\n",
    "best_params = random_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the best model\n",
    "best_xgb_model = XGBClassifier(**best_params, random_state=42)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance Pruning\n",
    "feature_importances = best_xgb_model.feature_importances_\n",
    "\n",
    "# Select top 100 important features\n",
    "top_n = 100\n",
    "important_feature_indices = feature_importances.argsort()[-top_n:][::-1] \n",
    "selected_features = X.columns[important_feature_indices]\n",
    "\n",
    "# Apply RFE on the reduced feature set\n",
    "X_train_top = X_train[selected_features]\n",
    "X_test_top = X_test[selected_features]\n",
    "\n",
    "xgb_model_rfe = XGBClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "# Select top 30 features\n",
    "rfe = RFE(xgb_model_rfe, n_features_to_select=30)  \n",
    "X_train_rfe = rfe.fit_transform(X_train_top, y_train)\n",
    "X_test_rfe = rfe.transform(X_test_top)\n",
    "\n",
    "# Retraining model with RFE-selected features\n",
    "best_xgb_model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = best_xgb_model.predict(X_test_rfe)\n",
    "\n",
    "evaluate_model(\"XGBoost (with Hyperparameter Tuning and Importance Pruning)\", y_test, y_pred, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.40      0.76      0.53      6907\n",
      "         tie       0.25      0.06      0.10      4480\n",
      "  win_team_1       0.60      0.42      0.50      9504\n",
      "\n",
      "    accuracy                           0.46     20891\n",
      "   macro avg       0.42      0.42      0.37     20891\n",
      "weighted avg       0.46      0.46      0.42     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train the model and Predicting outcomes\n",
    "nb_model.fit(X_train, y_train)\n",
    "y_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Gaussian Naive Bayes\",y_test, y_pred, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.47      0.27      0.35      6907\n",
      "         tie       0.24      0.83      0.38      4480\n",
      "  win_team_1       0.65      0.11      0.19      9504\n",
      "\n",
      "    accuracy                           0.32     20891\n",
      "   macro avg       0.45      0.40      0.30     20891\n",
      "weighted avg       0.50      0.32      0.28     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Bernoulli Naive Bayes Model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = bnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Bernoulli Naive Bayes\",y_test, y_pred, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.44      0.43      0.44      6907\n",
      "         tie       0.22      0.32      0.26      4480\n",
      "  win_team_1       0.57      0.46      0.51      9504\n",
      "\n",
      "    accuracy                           0.42     20891\n",
      "   macro avg       0.41      0.40      0.40     20891\n",
      "weighted avg       0.46      0.42      0.43     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train_no_negatives = X_train.drop(['home_team_net_transfer_record', 'away_team_net_transfer_record'], axis=1)\n",
    "X_test_no_negatives = X_test.drop(['home_team_net_transfer_record', 'away_team_net_transfer_record'], axis=1)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train_no_negatives, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = nb_model.predict(X_test_no_negatives)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Multinomial Naive Bayes\", y_test, y_pred, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complement Naive Bayes Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.42      0.52      0.46      6907\n",
      "         tie       0.28      0.06      0.09      4480\n",
      "  win_team_1       0.54      0.65      0.59      9504\n",
      "\n",
      "    accuracy                           0.48     20891\n",
      "   macro avg       0.41      0.41      0.38     20891\n",
      "weighted avg       0.44      0.48      0.44     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# Initialize the Complement Naive Bayes classifier\n",
    "nb_model = ComplementNB()\n",
    "\n",
    "# Train the model\n",
    "nb_model.fit(X_train_no_negatives, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = nb_model.predict(X_test_no_negatives)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Complement Naive Bayes\", y_test, y_pred, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (Linear Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# Initialize the SVM with a linear kernel\n",
    "svm_model = LinearSVC(max_iter=5000)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Linear SVM\", y_test, y_pred, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (Polynomial Kernel of Degree 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM with a polynomial kernel\n",
    "svm_model = SVC(kernel='poly', degree=3, max_iter=5000)  \n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Polynomial SVM\", y_test, y_pred, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (Radial Basis Function Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM with an RBF kernel\n",
    "svm_model = SVC(kernel='rbf', max_iter=5000)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"RBF Kernel SVM\", y_test, y_pred, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (Sigmoid Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the SVM with a sigmoid kernel\n",
    "svm_model = SVC(kernel='sigmoid',max_iter=5000)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting outcomes\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Sigmoid SVM\", y_test, y_pred, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours (Eucliding Distance, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours (Euclidian Distance, k = 5) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.45      0.56      0.50      6907\n",
      "         tie       0.25      0.19      0.22      4480\n",
      "  win_team_1       0.57      0.53      0.55      9504\n",
      "\n",
      "    accuracy                           0.47     20891\n",
      "   macro avg       0.42      0.43      0.42     20891\n",
      "weighted avg       0.46      0.47      0.46     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize k-NN with Euclidean distance and k=5 \n",
    "knn_euclidean = KNeighborsClassifier(metric='euclidean', n_neighbors=5)\n",
    "\n",
    "# Train the model with and Predict outcomes\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"K-Nearest Neighbours (Euclidian Distance, k = 5)\", y_test, y_pred_euclidean, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours (Manhattan Distance, k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours (Manhattan Distance, k = 5) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.45      0.56      0.50      6907\n",
      "         tie       0.26      0.19      0.22      4480\n",
      "  win_team_1       0.57      0.53      0.55      9504\n",
      "\n",
      "    accuracy                           0.47     20891\n",
      "   macro avg       0.43      0.43      0.42     20891\n",
      "weighted avg       0.46      0.47      0.46     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize k-NN with Manhattan distance and k=10\n",
    "knn_manhattan = KNeighborsClassifier(metric='manhattan', n_neighbors=5)\n",
    "\n",
    "# Train the model with Manhattan distance and k=10\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"K-Nearest Neighbours (Manhattan Distance, k = 5)\", y_test, y_pred_manhattan, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours (Eucliding Distance, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours (Euclidian Distance, k = 10) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.48      0.54      0.51      6907\n",
      "         tie       0.26      0.14      0.19      4480\n",
      "  win_team_1       0.56      0.63      0.59      9504\n",
      "\n",
      "    accuracy                           0.49     20891\n",
      "   macro avg       0.43      0.44      0.43     20891\n",
      "weighted avg       0.47      0.49      0.48     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize k-NN with Euclidean distance and k=10\n",
    "knn_euclidean = KNeighborsClassifier(metric='euclidean', n_neighbors=10)\n",
    "\n",
    "# Train the model with and Predict outcomes\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"K-Nearest Neighbours (Euclidian Distance, k = 10)\", y_test, y_pred_euclidean, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours (Manhattan Distance, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbours (Manhattan Distance, k = 10) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.45      0.56      0.50      6907\n",
      "         tie       0.26      0.19      0.22      4480\n",
      "  win_team_1       0.57      0.53      0.55      9504\n",
      "\n",
      "    accuracy                           0.47     20891\n",
      "   macro avg       0.43      0.43      0.42     20891\n",
      "weighted avg       0.46      0.47      0.46     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize k-NN with Manhattan distance and k=10\n",
    "knn_manhattan = KNeighborsClassifier(metric='manhattan', n_neighbors=5)\n",
    "\n",
    "# Train the model with Manhattan distance and k=10\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"K-Nearest Neighbours (Manhattan Distance, k = 10)\", y_test, y_pred_manhattan, label_encoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L2 Regularization (Ridge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (No Regularization) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.57      0.63      0.60      6907\n",
      "         tie       0.35      0.06      0.10      4480\n",
      "  win_team_1       0.61      0.80      0.69      9504\n",
      "\n",
      "    accuracy                           0.58     20891\n",
      "   macro avg       0.51      0.50      0.46     20891\n",
      "weighted avg       0.54      0.58      0.53     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Get Traning and Testing Data into the same sclae for better performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize Logistic Regression with balanced class weights and more iterations\n",
    "log_reg_balanced = LogisticRegression(solver='lbfgs', max_iter=10000, penalty=\"l2\")\n",
    "\n",
    "# Train the model and predict outcomes\n",
    "log_reg_balanced.fit(X_train_scaled, y_train)\n",
    "y_pred_balanced = log_reg_balanced.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Logistic Regression (No Regularization)\", y_test, y_pred_balanced, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L2 Regularization (Ridge) and balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (L2 Regularization) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.57      0.61      0.59      6907\n",
      "         tie       0.31      0.40      0.35      4480\n",
      "  win_team_1       0.69      0.56      0.62      9504\n",
      "\n",
      "    accuracy                           0.54     20891\n",
      "   macro avg       0.52      0.53      0.52     20891\n",
      "weighted avg       0.57      0.54      0.55     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression with L2 regularization using lbfgs solver\n",
    "log_reg_l2 = LogisticRegression(solver='lbfgs', penalty='l2', max_iter=10000, class_weight='balanced')\n",
    "\n",
    "# Train the model and predict outcomes\n",
    "log_reg_l2.fit(X_train_scaled, y_train)\n",
    "y_pred_l2 = log_reg_l2.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Logistic Regression (L2 Regularization)\", y_test, y_pred_l2, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L1 Regularization (Lasso) and balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (L1 Regularization) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.55      0.66      0.60      6907\n",
      "         tie       0.33      0.16      0.22      4480\n",
      "  win_team_1       0.65      0.71      0.68      9504\n",
      "\n",
      "    accuracy                           0.58     20891\n",
      "   macro avg       0.51      0.51      0.50     20891\n",
      "weighted avg       0.55      0.58      0.55     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression with L1 regularization using liblinear solver\n",
    "log_reg_l1 = LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Train the model and predict outcomes\n",
    "log_reg_l1.fit(X_train_scaled, y_train)\n",
    "y_pred_l1 = log_reg_l1.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Logistic Regression (L1 Regularization)\", y_test, y_pred_l1, label_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with ElasticNet Regularization and balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (ElasticNet Regularization) Model Evaluation:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " lose_team_1       0.57      0.61      0.59      6907\n",
      "         tie       0.31      0.40      0.35      4480\n",
      "  win_team_1       0.69      0.56      0.62      9504\n",
      "\n",
      "    accuracy                           0.54     20891\n",
      "   macro avg       0.52      0.52      0.52     20891\n",
      "weighted avg       0.57      0.54      0.55     20891\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression with ElasticNet regularization using saga solver\n",
    "log_reg_elasticnet = LogisticRegression(solver='saga', penalty='elasticnet', l1_ratio=0.5, max_iter=10000, class_weight='balanced')\n",
    "\n",
    "# Train the model and predict outcomes\n",
    "log_reg_elasticnet.fit(X_train_scaled, y_train)\n",
    "y_pred_elasticnet = log_reg_elasticnet.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(\"Logistic Regression (ElasticNet Regularization)\", y_test, y_pred_elasticnet, label_encoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
